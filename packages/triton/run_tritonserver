#!/bin/bash

# cd "${0%/*}"
HTTP_PORT=${HTTP_PORT:-8080}
GRPC_PORT=${GRPC_PORT:-8080}
TRITON_HTTP_PORT=${TRITON_HTTP_PORT:-8000}
TRITON_GRPC_PORT=${TRITON_GRPC_PORT:-8001}
TRITON_METRIC_PORT=${TRITON_METRIC_PORT:-8002}

mv /models/archived $MODEL_DIR/$MODEL_NAME

/opt/tritonserver/nvidia_entrypoint.sh && tritonserver \
  --model-store=$MODEL_DIR \
  --http-port=${TRITON_HTTP_PORT:-8000} \
  --grpc-port=${TRITON_GRPC_PORT:-8001} \
  --allow-grpc=true \
  --allow-http=true \
  --allow-metrics=true \
  --allow-gpu-metrics=true \
  --strict-readiness=true \
  --log-info=true \
  $@ & \
python3 -m tritonserver.inferencer \
  --model_name=$MODEL_NAME \
  --model_dir=$MODEL_DIR \
  --http_port=$HTTP_PORT \
  --grpc_port=$GRPC_PORT

# without `config.pbtxt`: --strict-model-config = false & savedmodel

  # tritonserver \
  # --model-repository=$MODEL_DIR \
  # --grpc-port=$TRITON_GRPC_PORT \
  # --http-port=$TRITON_HTTP_PORT \
  # --allow-grpc=true \
  # --allow-http=true \
  # $@

"$@"